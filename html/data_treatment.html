# %%
# Librer√≠as
import os
import pandas as pd
import numpy as np
import openpyxl
import missingno as msno
import seaborn as sns
import matplotlib.pyplot as plt
import sys
from sklearn.model_selection import train_test_split

# %%
import scipy.stats as ss

# %% [markdown]
# Centrado en el tratamiento de los datos. El procesamiento de datos es una etapa clave para garantizar que los algoritmos de modelado funcionen de manera eficiente y precisa. A continuaci√≥n, se detalla el enfoque estructurado para preparar y transformar los datos:
# 
# Va a seguir la siguiente estructura:
# 
# 1.‚Å† ‚Å†Separaci√≥n Train y Test: La divisi√≥n de los datos en conjuntos de entrenamiento y prueba es esencial para evaluar la capacidad del modelo para generalizar a nuevos datos.
# 2.‚Å† ‚Å†Tratamiento de outliers: Los valores at√≠picos pueden sesgar los resultados del modelo o influir negativamente en el rendimiento.
# 
# 3.‚Å† ‚Å†An√°lisis de relaciones entre variables 
# 
#     3.1. Matriz de correlaci√≥n. Calcula las correlaciones entre variables num√©ricas usando el coeficiente de Pearson. Ayuda a identificar multicolinealidad, es decir, relaciones muy fuertes entre variables independientes, lo que puede afectar el rendimiento del modelo.
# 
#     3.2. Matriz de Cramer. Para evaluar relaciones entre variables categ√≥ricas, se utiliza la V de Cramer, un indicador de asociaci√≥n que var√≠a entre 0 (sin relaci√≥n) y 1 (relaci√≥n perfecta).
#     Esto permite identificar qu√© variables categ√≥ricas tienen una relaci√≥n significativa entre s√≠ o con la variable objetivo, ayudando a decidir si es necesario combinarlas, transformarlas o eliminarlas.
# 
# 4.‚Å† ‚Å†WOE y Valor de la Informaci√≥n (Information Value). Ayuda a evaluar la calidad predictiva de las variables

# %%
# Obtener la ruta del directorio de trabajo actual
ruta_actual = os.getcwd()
ruta_actual

# %%
#lectura del dataset
data = '../data/interim/df_loans_N1.csv'
df_loans = pd.read_csv(data)
df_loans.head()

# %%
#Tambi√©n se va a importar el diccionario de datos del dataset 

data_description = pd.read_excel('../data/columns_description.xlsx') 
data_description.head()

# %%
#Se elimina la primera columna porque se repite el √≠ndice.
df_loans.drop(columns=['Unnamed: 0'], inplace=True)

# %%

from sklearn.model_selection import train_test_split
X_df_loans_train, X_df_loans_test, y_df_loans_train, y_df_loans_test = train_test_split(df_loans.drop('TARGET',axis=1), 
                                                                     df_loans['TARGET'], 
                                                                     stratify=df_loans['TARGET'], 
                                                                     test_size=0.2)
df_loans_train = pd.concat([X_df_loans_train, y_df_loans_train],axis=1)
df_loans_test = pd.concat([X_df_loans_test, y_df_loans_test],axis=1)

# %% [markdown]
# 
# Para nuestro conjunto de datos, se ha decidido utilizar el 20% del total para el entrenamiento, considerando que, al tratarse de un dataset grande, esta proporci√≥n es suficiente para entrenar el modelo de manera efectiva y reservar una cantidad significativa de datos para evaluar su desempe√±o.
# 
# Esta elecci√≥n tambi√©n ayuda a mitigar problemas como el sobreajuste, ya que un conjunto de entrenamiento demasiado grande podr√≠a llevar al modelo a ajustarse en exceso a los datos, comprometiendo su capacidad de generalizaci√≥n frente a datos nuevos.
# 
# Inicialmente, la distribuci√≥n 80/20 parece adecuada para las necesidades actuales, aunque no se descarta ajustar esta proporci√≥n en el futuro, por ejemplo, con un esquema 70/15/15 que incluya un conjunto dedicado a la validaci√≥n cruzada.

# %%
# Proporci√≥n en el conjunto original
print("Distribuci√≥n original:")
print(df_loans['TARGET'].value_counts(normalize=True))

# Proporci√≥n en el conjunto de entrenamiento
print("\nDistribuci√≥n en el conjunto de entrenamiento:")
print(y_df_loans_train.value_counts(normalize=True))  

# Proporci√≥n en el conjunto de prueba
print("\nDistribuci√≥n en el conjunto de prueba:")
print(y_df_loans_test.value_counts(normalize=True))  


# %% [markdown]
# Se puede afirmar que la estratificaci√≥n ha sido realizada de manera correcta ya que se preservan las proporciones de la variable objetivo para dataset de train como de test.

# %% [markdown]
# Se debe tener presente que la variable objetivo est√° muy desbalanceada. En este caso implica que hay muy pocos clientes que tengan dificultades en el pago del pr√©stamo (8,07%) frente a una gr√°n proporci√≥n de los clientes que no presentan dificultades a la hora de pagar (91.93%). Por ello, es posible que, previo a la ejecuci√≥n del modelado, se requiera un procesamiento de oversampling para compensar de cierta manera ese desbalance.

# %% [markdown]
# ## Tratamiento de Outliers

# %%
#primero se seleccionan las num√©ricas sin las buleanas
columnas_numericas_sin_booleanas = [col for col in df_loans_train.select_dtypes(include=[np.number]).columns if df_loans_train[col].dtype != 'bool']

# Ver los nombres de las columnas
print(columnas_numericas_sin_booleanas)


# %%
pd.set_option('display.max_rows', None)

# %%
fa.get_deviation_of_mean_perc(df_loans_train, columnas_numericas_sin_booleanas, target='TARGET', multiplier=3)

# %% [markdown]
# Dada la naturaleza del an√°lisis actual y la distribuci√≥n de los valores outliers identificados, no se considera prioritario tratarlos en esta etapa inicial. 
# Muchas de las variables afectadas por valores extremos, como las banderas documentales o indicadores espec√≠ficos, tienen un impacto relativamente peque√±o en el conjunto total de datos (proporciones menores al 10%). Adem√°s, su naturaleza discreta sugiere que estos valores podr√≠an representar informaci√≥n √∫til en lugar de ruido o errores.
# 
# No obstante, no se descarta la posibilidad de abordar los outliers en fases futuras del an√°lisis, especialmente si afectan la estabilidad o el desempe√±o de modelos predictivos posteriores. El enfoque en este momento se centra en preservar la integridad de los datos y explorar c√≥mo estas anomal√≠as interact√∫an con la variable objetivo antes de tomar decisiones de tratamiento definitivo.

# %% [markdown]
# ### Tratamiento de Valores Nulos 

# %%
def reemplazar_nulos_por_desconocido(df):
    """
    Reemplaza los valores nulos en el DataFrame por "desconocido".
    
    :param df: DataFrame con los datos a procesar.
    :return: DataFrame con los valores nulos reemplazados por "desconocido".
    """
    return df.fillna("desconocido")


# %%
reemplazar_nulos_por_desconocido(df_loans_train)


# %% [markdown]
# ## An√°lisis entre Variables 

# %% [markdown]
# ### Relaciones entre variables num√©ricas: Correlation Matrix

# %%
boolean_columns = df_loans.columns[df_loans.isin([0, 1]).all()]

matrix_corr = pd.concat([df_loans_train.select_dtypes('number').drop(boolean_columns, axis=1), df_loans_train['TARGET']], axis=1).corr(method='pearson')
matrix_corr

# %%
# Pintar la matriz de correlaci√≥n con seaborn
plt.figure(figsize=(10, 8))  # Tama√±o de la figura
sns.heatmap(matrix_corr, cmap='coolwarm', fmt='.2f', cbar=True, square=True)

# A√±adir t√≠tulo
plt.title('Matriz de Correlaci√≥n')

# Mostrar el gr√°fico
plt.show()

# %% [markdown]
# ##### CONCLUSIONES DE LA MATRIZ DE CORRELACI√ìN
# 
# 
# 

# %% [markdown]
# ### Relaciones entre variables categ√≥ricas: Cramers V matrix
# 
# La matriz de Cram√©r se usa para medir la asociaci√≥n entre dos variables categ√≥ricas. Su coeficiente, Cram√©r's V, va de 0 (sin relaci√≥n) a 1 (relaci√≥n perfecta). 
# 
# En este caso es de gran utilidad para analizar la dependencia en tablas de contingencia.
# 
# Para nuestro dataset de concesi√≥n de pr√©stamo permitir√° evaluar la relaci√≥n entre variabes como "g√©nero", "tipo de casa", "materiales de la vivienda", "tipo de solicitante" etc con la variable TARGET que toma valor 1 si tuvo alg√∫n retraso en alguno de los pagos de la devoluci√≥n del pr√©stamo y 0 en cualquier otro caso.
# 
# Por ello un valor alto de Cramer¬¥s matrix sugiere que esas variables tienen un impacto en la probabilidad de impago.
# 
# Para este caso solo se tenr√°n en cuenta estrictamente las variables con mas de 2 categor√≠as para llevar a cabo el an√°lisis ya que sumado a lo ya mencionado las variables buleanas se encuentran bastante debalanceadas por lo que se ha decidido evitar incluirlas ya que en estos casos, Cram√©r's V podr√≠a no ofrecerte una visi√≥n clara o √∫til de la relaci√≥n.

# %%
# Seleccionar las columnas num√©ricas que tienen solo valores 0 y 1
boolean_columns = df_loans.columns[df_loans.isin([0, 1]).all()]

# Convertir las columnas seleccionadas a tipo booleano
df_loans[boolean_columns] = df_loans[boolean_columns].astype(bool)

# Verifica si las columnas han sido convertidas correctamente
print(df_loans.dtypes)

# %%
# Seleccionamos las columnas booleanas
boolean_columns = df_loans.select_dtypes(include=['bool']).columns

# Crear gr√°ficos de barras para cada variable booleana
for col in boolean_columns:
    plt.figure(figsize=(6, 4))
    df_loans[col].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
    plt.title(f'Distribuci√≥n de la variable {col}')
    plt.xlabel(col)
    plt.ylabel('Frecuencia')
    plt.xticks(ticks=[0, 1], labels=['False', 'True'], rotation=0)
    plt.show()

# %%
# Calcular la matriz de correlaci√≥n para las variables booleanas
corr_matrix = df_loans[boolean_columns].corr()

# Crear un mapa de calor para la matriz de correlaci√≥n
plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5, vmin=-1, vmax=1)

# T√≠tulo en negrita y m√°s grande
plt.title('Mapa de Calor de la Matriz de Correlaci√≥n para Variables Booleanas', fontweight='bold', fontsize=30)
plt.show()

# %% [markdown]
# ##### COMENTARIO DE LA MATRIZ 
# 
# La matriz de correlaci√≥n presentada muestra c√≥mo se relacionan entre s√≠ diferentes variables booleanas del conjunto de datos, incluyendo la variable objetivo TARGET, que indica si un cliente se retras√≥ en el pago. Cada celda refleja la fuerza de la correlaci√≥n entre las variables.
# 
# Los valores m√°s cercanos a 1 (en tonos oscuros) representan una relaci√≥n fuerte, mientras que los valores cercanos a 0 (en tonos claros) indican poca o ninguna correlaci√≥n.
# Se observa que las variables REG_REGION_NOT_WORK_REGION y LIVE_REGION_NOT_WORK_REGION presentan una correlaci√≥n moderada (0.45), lo que sugiere que trabajar y vivir en regiones diferentes podr√≠a estar asociado en el conjunto de datos.
# 
# En cuanto a TARGET, la mayor√≠a de las correlaciones son bajas, con valores menores a 0.1. Esto indica que las variables booleanas analizadas tienen poca influencia directa sobre el retraso en los pagos.
# 
# Variables como FLAG_EMAIL, FLAG_PHONE, y FLAG_DOCUMENT_X presentan correlaciones muy bajas con TARGET, lo que sugiere que estas caracter√≠sticas no son determinantes en el comportamiento de pago del cliente.
# 
# En general, las correlaciones en la matriz son d√©biles, lo cual es com√∫n en an√°lisis de variables booleanas. Sin embargo, es importante evaluar estas relaciones en combinaci√≥n con otras variables para capturar patrones m√°s complejos y mejorar el modelo predictivo.

# %%
# Llamar a la funci√≥n y guardar los resultados en variables
col_bool, col_cat, col_num = fa.categorizar_columnas(df_loans)

# %%
# Asegurarte de incluir la variable TARGET
col_cat_str = list(X_df_loans_train[col_cat].select_dtypes('object').columns.values)
col_cat_str

# %%
# Agregar manualmente 'TARGET' si no est√° ya en la lista
if 'TARGET' not in col_cat_str:
    col_cat_str.append('TARGET')

# Crear una matriz vac√≠a para almacenar los valores de Cram√©r's V
cramers_matrix = pd.DataFrame(index=col_cat_str, columns=col_cat_str)

# Rellenar la matriz con los valores de Cram√©r's V
for col1 in col_cat_str:
    for col2 in col_cat_str:
        # Crear la tabla de contingencia
        confusion_matrix = pd.crosstab(df_loans_train[col1], df_loans_train[col2])
        # Calcular Cram√©r's V
        cramers_matrix.loc[col1, col2] = fa.cramers_v(confusion_matrix.values)


# %%
# Convertir los valores a float para que se puedan leer adecuadamente
cramers_matrix = cramers_matrix.astype(float)

# Imprimir la matriz de Cram√©r's V
print(cramers_matrix)

# %%
# Ajustar el esquema de colores y mostrar la matriz
plt.figure(figsize=(15, 8))
sns.set(style='white')  # Estilo m√°s limpio
# Crear el mapa de calor con colores rosas
ax = sns.heatmap(cramers_matrix, annot=True, fmt='.3f', cmap='Reds', cbar=True)

# Iterar sobre las anotaciones para poner en negrita los valores altos
for text in ax.texts:
    if float(text.get_text()) > 0.5:  # Umbral de correlaci√≥n alta
        text.set_fontweight('bold')

# A√±adir t√≠tulo y personalizarlo
plt.title('Cram√©r\'s V Matrix', fontdict={'size': 17},fontsize=30)
plt.show()


# %% [markdown]
# ##### COMENTARIO DE LA MATRIZ 
# La matriz de Cram√©r's V presentada analiza la relaci√≥n entre diferentes variables categ√≥ricas del conjunto de datos, incluyendo la variable objetivo TARGET, que indica si un cliente se retras√≥ en el pago. Cada celda muestra la intensidad de la asociaci√≥n entre las variables:
# 
# Los valores m√°s cercanos a 1 (en tonos oscuros) indican asociaciones m√°s fuertes, mientras que los valores pr√≥ximos a 0 (en tonos claros) reflejan poca o ninguna relaci√≥n.
# Se destaca una relaci√≥n moderada entre CODE_GENDER y OCCUPATION_TYPE (0.405), lo que podr√≠a indicar que ciertas ocupaciones son m√°s comunes entre hombres o mujeres.
# En cuanto a TARGET, las asociaciones son bajas en general, siendo ORGANIZATION_TYPE (0.072) y OCCUPATION_TYPE (0.082) las variables con mayor relaci√≥n. Esto sugiere que la ocupaci√≥n y el tipo de organizaci√≥n donde trabaja el cliente podr√≠an tener una ligera influencia en el riesgo de retraso en los pagos.
# Variables como FLAG_OWN_CAR y HOUSE_TYPE_MODE presentan correlaciones muy bajas con TARGET, indicando que estas caracter√≠sticas no est√°n significativamente asociadas con el comportamiento de pago.
# En general, las asociaciones en la matriz son d√©biles, lo que es com√∫n en datos categ√≥ricos. Sin embargo, las relaciones m√°s significativas deben ser evaluadas en contexto, ya que pueden aportar informaci√≥n valiosa para el modelo predictivo y la toma de decisiones.

# %% [markdown]
# ## WOE y Valor de la Informaci√≥n (Information Value)

# %% [markdown]
# #### WOE (Weight of Evidence)

# %% [markdown]
# **1- DEFINCI√ìN**  
# 
# WOE (The Weight of Evidence) indica el poder predictivo de una variable independiente en relaci√≥n con la variable dependiente. Originado en el √°mbito de la evaluaci√≥n crediticia, generalmente se describe como una medida de la capacidad de separaci√≥n entre buenos y malos clientes. Los "Malos Clientes" se refieren a aquellos que incumplieron con el pago de un pr√©stamo (en este caso ser√≠a aquellos clienten que tienen dificultades de pago), mientras que los "Buenos Clientes" son los que lo reembolsaron, es decir, aquellos que no tuvieron dificultades durante el reembolso.
# 
# Es decir, el WOE mide qu√© tan bien cada grupo de atributos puede predecir el valor deseado de la variable dependiente.
#   
# 
# 
# - Distribution of Goods - % de buenos clientes (aquellos sin dificultad de pago) en un grupo particular.
# - Distribution of Bads - % de malos clientes (aquellos con dificultad de pago) en un grupo particular.
# - ln - Logaritmo natural
# 
# 
# **2- PASOS**
# 
# **Paso 1. Verifica el tipo de variable:**
# 
#     Si es continua
# Divide los datos en 10 grupos (o menos, dependiendo de la distribuci√≥n). Idealmente, cada grupo debe contener al menos el 5% de los casos.   
# 
#     Si es categ√≥rica
# 
# Omite este paso.
# 
# **Paso 2. Cuenta los valores buenos y malos en cada grupo:**  
# 
# "Buenos" se refieren a eventos positivos (por ejemplo, clientes que pagaron).  
# "Malos" se refieren a eventos negativos (por ejemplo, clientes que incumplieron).  
# 
# **Paso 3. Calcula el % de buenos y malos**  Mediante la f√≥rmula
# 
# **Paso 4. Calcula el WOE para cada grupo**
# 
# **Paso 5. Agrupa categor√≠as con WOE similares:**  Si una variable tiene m√∫ltiples grupos, puedes combinarlos si tienen valores de WOE similares, ya que su comportamiento al predecir la variable objetivo ser√° comparable. Esto simplifica el modelo y mejora la interpretabilidad.
# 
# 
# **3- Interpretaci√≥n de posibles Resultados:**
# 
# - Un WOE positivo indica que la distribuci√≥n de los buenos (clientes que pagaron el pr√©stamo) es mayor que la de los malos (clientes que incumplieron). Esto sugiere que esa categor√≠a o grupo tiene una proporci√≥n relativamente alta de eventos positivos.
# 
# - Un WOE negativo indica que la distribuci√≥n de los buenos es menor que la de los malos. Esto implica que esa categor√≠a o grupo tiene una proporci√≥n relativamente alta de eventos negativos.
# 
# La interpretaci√≥n del WOE es esencial para evaluar la relaci√≥n entre una variable independiente y el comportamiento de la variable dependiente (la dificultad de pago).
# 
# 
# 
# 
# 
# 

# %% [markdown]
# #### Valor de la Informaci√≥n (Information Value)

# %% [markdown]
# El Valor de la Informaci√≥n (IV) y el Peso de la Evidencia (WOE) tienen una conexi√≥n directa y significativa.
# 
# El IV es un m√©todo de an√°lisis exploratorio que ayuda a identificar qu√© variable en un conjunto de datos posee capacidad predictiva o impacto sobre el valor de una variable dependiente binaria espec√≠fica (0 o 1). En otras palabras, es un indicador num√©rico que mide la capacidad general de una variable independiente 
# ùëã para predecir el comportamiento de la variable objetivo.

# %% [markdown]
# El Valor de la Informaci√≥n (IV) resulta √∫til para reducir el n√∫mero de variables empleadas al crear un modelo de Regresi√≥n Log√≠stica, particularmente cuando existen muchas variables potenciales. El IV examina cada variable independiente de manera individual, sin tener en cuenta las otras variables predictoras. Dependiendo de los valores de IV de cada variable, se puede evaluar su poder predictivo utilizando la siguiente l√≥gica:  
# 
# 
# - IV < 0.02: La variable no tiene poder predictivo (no es √∫til para el modelo).  
# - 0.02 ‚â§ IV < 0.1: La variable tiene un poder predictivo d√©bil.    
# - 0.1 ‚â§ IV < 0.3: La variable tiene un poder predictivo moderado.  
# - IV ‚â• 0.3: La variable tiene un poder predictivo fuerte (puede ser muy influyente en el modelo).  
# 
# Este enfoque permite priorizar las variables m√°s relevantes y descartar aquellas que no aportan valor significativo al modelo, optimizando tanto su precisi√≥n como su eficiencia.

# %% [markdown]
# #### 1- Variables catgoricas (WOE y IV)

# %%
# Funci√≥n para calcular WOE e IV para variables categ√≥ricas
def calculate_woe_iv_cat(df, feature, target):
    """
    Calcula el Weight of Evidence (WoE) y el Information Value (IV) para una variable categ√≥rica.
    
    Args:
        df (DataFrame): DataFrame que contiene los datos.
        feature (str): Nombre de la variable categ√≥rica.
        target (str): Nombre de la variable objetivo (debe ser binaria: 0/1).

    Returns:
        DataFrame: Tabla con los valores de WoE e IV para cada categor√≠a y el IV total.
    """
    # Crear una tabla de contingencia para la variable actual
    grouped = df.groupby(feature)[target].value_counts().unstack(fill_value=0)
    
    # Calcular las proporciones de buenos y malos por cada categor√≠a
    grouped['good_pct'] = grouped[1] / grouped[1].sum()
    grouped['bad_pct'] = grouped[0] / grouped[0].sum()

    # Agregar un peque√±o valor (epsilon) para evitar divisi√≥n por 0
    epsilon = 1e-6
    grouped['good_pct'] += epsilon
    grouped['bad_pct'] += epsilon

    # Calcular el WOE
    grouped['WOE'] = np.log(grouped['bad_pct'] / grouped['good_pct'])

    # Calcular el IV para cada categor√≠a
    grouped['IV'] = (grouped['bad_pct'] - grouped['good_pct']) * grouped['WOE']

    # Calcular el IV total
    iv_total = grouped['IV'].sum()

    # Agregar una columna con el nombre de la variable y el IV total
    grouped['Feature'] = feature
    grouped['IV_Total'] = iv_total

    return grouped[['WOE', 'IV', 'Feature', 'IV_Total']]

# %%
# Calcular y mostrar el WOE e IV para cada variable categ√≥rica
categoric_val = df_loans.select_dtypes(['category', 'object']).columns

for var in categoric_val:
    print(f"Resultados para la variable categ√≥rica: {var}")
    try:
        # Calcular WOE e IV
        woe_iv = calculate_woe_iv_cat(df_loans, var, 'TARGET')
        
        # Imprimir los resultados
        print(woe_iv[['WOE', 'IV']])  # Muestra solo las columnas WOE e IV por categor√≠a
        print(f"IV Total para {var}: {woe_iv['IV_Total'].iloc[0]:.4f}")  # IV Total
    except Exception as e:
        print(f"Error procesando la variable {var}: {e}")
        
    if (woe_iv['IV_Total'] < 0.02).any():
        print(f"La variable {var} no es relevante para el modelo.")
    elif (woe_iv['IV_Total'] >= 0.02).any() and (woe_iv['IV_Total'] < 0.1).any():
        print(f"La variable {var} tiene un impacto moderado en el modelo.")
    else:
        print(f"La variable {var} tiene un impacto fuerte en el modelo.")
    print("\n" + "-" * 50 + "\n")

# %% [markdown]
# #### Resultados del WOE y DE IV de variables categ√≥ricas
# 
# A partir del Information value se puede analizar el impacto que tiene una variable en el modelo. En el caso de las variables categ√≥ricas no se encuentra ninguna que tenga un impacto grande en el modelo. Destaca que la mayor√≠a de las variables categ√≥ricas no son relevantes para el modelo. Asimismo, se han encontrado varias variables que se consideran moderadamente relevantes. Estas variables son CODE_GENDER, NAME_INCOME_TYPE, NAME_FAMILY_STATUS, OCCUPATION_TYPE.

# %% [markdown]
# **Variable CODE_GENDER**

# %% [markdown]
# - G√©nero F (Femenino) tiene un WOE positivo de **0.1543**, lo que indica que las observaciones de esta categor√≠a tienen una mayor probabilidad de ser "buenas" (valor `1` en `TARGET`) en comparaci√≥n con las malas.    
# 
# - M (Masculino) tiene un WOE negativo de **-0.2509**, lo que indica que las observaciones de esta categor√≠a tienen una mayor probabilidad de ser "malas" (valor `0` en `TARGET`) en comparaci√≥n con las buenas.  
# 
# - XNA tiene un WOE extremadamente alto de **2.7180**, lo que podr√≠a sugerir que es una categor√≠a con muy pocas observaciones o un `valor at√≠pico`. Este valor podr√≠a necesitar un an√°lisis adicional, ya que podr√≠a indicar un comportamiento inusual en los datos.
# 

# %% [markdown]
# **Variable NAME_INCOME_TYPE**

# %% [markdown]
# El an√°lisis de **WOE (Weight of Evidence)** para la variable **NAME_INCOME_TYPE** muestra lo siguiente:
# 
# - **Businessman** tiene un WOE muy alto de **3.5939**, lo que indica una gran probabilidad de ser "bueno" (valor `1` en `TARGET`).
# - **Student** tambi√©n tiene un WOE positivo de **4.1694**, sugiriendo una mayor probabilidad de ser "bueno".
# - **Pensioner** y **State servant** tienen WOE positivos de **0.4334** y **0.3633** respectivamente, lo que tambi√©n se√±ala una mayor probabilidad de ser "bueno".
#   
# Por otro lado, las categor√≠as con WOE negativo son:
# - **Maternity leave** tiene un WOE negativo de **-1.9493**, lo que sugiere una alta probabilidad de ser "mala" (valor `0` en `TARGET`).
# - **Unemployed** tiene un WOE negativo de **-1.8560**, lo que indica una mayor probabilidad de ser "mala".
# - **Working** tiene un WOE negativo de **-0.1887**, con un impacto negativo m√°s leve en la probabilidad de ser "mala".
#   
# En resumen, **Businessman** y **Student** est√°n asociadas con una alta probabilidad de ser "buenos", mientras que **Maternity leave** y **Unemployed** est√°n asociadas con una mayor probabilidad de ser "malos". **Pensioner** y **State servant** tienen una probabilidad moderada de ser "buenos", mientras que **Working** muestra una probabilidad m√°s equilibrada. El IV total para esta variable es **0.0587**, lo que indica una importancia moderada de esta variable para predecir el **TARGET**.

# %% [markdown]
# **NAME_FAMILY_STATUS**

# %% [markdown]
# El an√°lisis de **WOE (Weight of Evidence)** para la variable **NAME_FAMILY_STATUS** muestra que:
# 
# - **Widow** tiene un WOE positivo de **0.3506**, lo que sugiere que las personas en esta categor√≠a tienen una mayor probabilidad de ser "buenas" (valor `1` en `TARGET`).
# - **Married** tiene un WOE positivo de **0.0712**, lo que tambi√©n indica una mayor probabilidad de ser "bueno".
# - **Unknown** tiene un WOE extremadamente alto de **2.0888**, aunque este valor es muy at√≠pico, lo que puede indicar una categor√≠a con pocos registros o casos at√≠picos.
# 
# Por otro lado, las categor√≠as con WOE negativo son:
# - **Civil marriage** tiene un WOE negativo de **-0.2291**, lo que indica una mayor probabilidad de ser "mala" (valor `0` en `TARGET`).
# - **Single / not married** tiene un WOE negativo de **-0.2137**, lo que tambi√©n indica una mayor probabilidad de ser "mala".
# - **Separated** tiene un WOE ligeramente negativo de **-0.0162**, pero la diferencia es peque√±a.
# 
# En resumen, **Widow** y **Married** est√°n asociadas con una mayor probabilidad de ser "buenas", mientras que **Civil marriage** y **Single / not married** est√°n asociadas con una mayor probabilidad de ser "malas". **Unknown** tiene un WOE elevado, lo que podr√≠a necesitar una revisi√≥n m√°s detallada.

# %% [markdown]
# **VARIABLE OCCUPATION_TYPE**

# %% [markdown]
# El an√°lisis de **WOE (Weight of Evidence)** para la variable **OCCUPATION_TYPE** muestra que:
# 
# - **Accountants** tiene un WOE positivo de **0.6406**, lo que sugiere que esta ocupaci√≥n tiene una mayor probabilidad de ser "buena" (valor `1` en `TARGET`).
# - **Managers** tambi√©n tiene un WOE positivo de **0.3740**, indicando una probabilidad de ser "bueno" m√°s alta.
# - **Core staff** tiene un WOE positivo de **0.3587**, lo que implica una mayor probabilidad de ser "bueno".
#   
# En cuanto a las ocupaciones con WOE negativo:
# - **Low-skill Laborers** tiene un WOE muy bajo de **-0.7652**, indicando que esta ocupaci√≥n tiene una alta probabilidad de ser "mala" (valor `0` en `TARGET`).
# - **Drivers**, **Laborers**, y **Waiters/barmen staff** tienen WOE negativos, lo que indica que tienen una probabilidad mayor de ser "malas".
#   
# En resumen, las ocupaciones con WOE positivo, como **Accountants** y **Managers**, est√°n asociadas con una mayor probabilidad de "buenas", mientras que las ocupaciones con WOE negativo, como **Low-skill Laborers**, est√°n asociadas con una mayor probabilidad de "malas".

# %%
df_loans_train.to_csv('../data/interim/df_loans_train.csv') # luego se separar√° nuevamente en X e y a la hora de modelar.
df_loans_test.to_csv('../data/interim/df_loans_test.csv') # lo mismo para el test, se  va a separar luego para no guardar dos csv separados.



